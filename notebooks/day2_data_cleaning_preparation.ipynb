{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5508c77",
   "metadata": {},
   "source": [
    "# üìÖ Day 2 ‚Äî Data Cleaning & Preparation\n",
    "\n",
    "*RTU Data Analysis & Visualization CPD course*\n",
    "\n",
    "**üìö Instruction (3h)**  \n",
    "- üßπ Handling missing values  \n",
    "- üóë Removing duplicates  \n",
    "- üîÑ Data type conversion  \n",
    "- üìÖ Parsing dates  \n",
    "- üèó Feature engineering basics  \n",
    "- üîó Combining datasets  \n",
    "- üè∑ Intro to categorical encoding  \n",
    "\n",
    "**üõ† Practical (1h)**  \n",
    "- üßΩ Clean a messy dataset  \n",
    "- üîÄ Merge with a secondary dataset  \n",
    "\n",
    "**üîÑ Reflection (1h)**  \n",
    "- üßê Review: common pitfalls in cleaning  \n",
    "- üí¨ Discuss real-world cleaning challenges  \n",
    "- üìù Recap exercise: identify cleaning steps for a small example dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea319510",
   "metadata": {},
   "source": [
    "## üéØ Goals for the Day\n",
    "- Strengthen Python basics (functions, loops, if/else, file handling)\n",
    "- Learn to process raw messy text files into usable form\n",
    "- Apply pandas methods to clean incomplete/messy data\n",
    "- Merge multiple datasets into a single unified dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16816f",
   "metadata": {},
   "source": [
    "## üí° Motivation / Explanation\n",
    "\n",
    "### Introduction to Data Cleaning and Preparation\n",
    "\n",
    "- **Why cleaning is critical before analysis**\n",
    "  - Raw data is almost never ready for direct analysis\n",
    "  - Errors, inconsistencies, and missing information can distort results\n",
    "  - Proper cleaning ensures reliability, reproducibility, and trust in analysis outcomes\n",
    "\n",
    "- **Real-world examples of messy data**\n",
    "  - üßπ Handling missing values - Weather records with missing timestamps or corrupt values\n",
    "  - üó≥Ô∏è Survey responses with inconsistent categories (e.g., \"Male\", \"male\", \"M\")\n",
    "  - üóë Removing duplicates - Financial transactions with duplicate entries\n",
    "  - üóë Log files with noise lines, system messages, or broken encodings\n",
    "  - üìÖ Parsing dates - Event logs with inconsistent timestamp formats\n",
    "  - üîÑ Data type conversion - User age recorded as text instead of numbers\n",
    "\n",
    "> Think of data cleaning as *‚Äúwashing vegetables before cooking‚Äù* ‚Äî not exciting, but essential for a good meal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9faea",
   "metadata": {},
   "source": [
    "## Part 1: Python Fundamentals for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, platform, os, io, shutil, zipfile, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    requests = None\n",
    "    print('requests not installed. Install with `%pip install requests`.')\n",
    "\n",
    "print('pandas:', pd.__version__)\n",
    "print('Python :', sys.version.split()[0])\n",
    "print('Runtime:', platform.platform())\n",
    "print('Now    :', datetime.now().isoformat(timespec='seconds'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523062c",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name: str = 'student') -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "print(greet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url: str, target_folder: str | Path = 'sample_data') -> Path:\n",
    "    target = Path(target_folder)\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    filename = url.split('/')[-1]\n",
    "    zip_path = target / filename\n",
    "    if requests is None:\n",
    "        raise RuntimeError('requests required.')\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(target)\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369862d6",
   "metadata": {},
   "source": [
    "**Practice dataset for Part 1:** `latvia_meteo_1925_messy.zip` (5 text files)\n",
    "\n",
    "- URL: https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_messy.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153f8c3",
   "metadata": {},
   "source": [
    "### üìÇ File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_lines(path: Path):\n",
    "    with path.open('r', encoding='utf-8', errors='replace') as f:\n",
    "        for line in f:\n",
    "            yield line.rstrip('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc72f9",
   "metadata": {},
   "source": [
    "### üîÑ For Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(path: Path) -> tuple[int, int]:\n",
    "    total, nonempty = 0, 0\n",
    "    for line in iter_lines(path):\n",
    "        total += 1\n",
    "        if line.strip():\n",
    "            nonempty += 1\n",
    "    return total, nonempty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc944702",
   "metadata": {},
   "source": [
    "### üå≥ If / Else Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_line(line: str) -> bool:\n",
    "    s = line.strip()\n",
    "    if not s: return False\n",
    "    if s.startswith('#'): return False\n",
    "    if len(s) < 5: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82debebc",
   "metadata": {},
   "source": [
    "### üßπ Building a Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(path: Path) -> tuple[int, int]:\n",
    "    out_good = path.with_suffix('.good.txt')\n",
    "    out_bad = path.with_suffix('.bad.txt')\n",
    "    good, bad = 0, 0\n",
    "    with path.open('r', encoding='utf-8', errors='replace') as fin, \\\n",
    "         out_good.open('w') as fg, out_bad.open('w') as fb:\n",
    "        for line in fin:\n",
    "            if is_good_line(line):\n",
    "                fg.write(line)\n",
    "                good += 1\n",
    "            else:\n",
    "                fb.write(line)\n",
    "                bad += 1\n",
    "    return good, bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e34eb",
   "metadata": {},
   "source": [
    "### üìÅ Extending to Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_files(folder: Path):\n",
    "    results = {}\n",
    "    for file in folder.glob('*.txt'):\n",
    "        results[file] = clean_file(file)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a338f",
   "metadata": {},
   "source": [
    "### üì• Loading Cleaned Files into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_file(path: Path) -> pd.DataFrame:\n",
    "    with path.open('r') as f:\n",
    "        lines = [l.strip().split() for l in f if l.strip()]\n",
    "    maxlen = max(len(r) for r in lines) if lines else 0\n",
    "    cols = [f'col{i+1}' for i in range(maxlen)] if maxlen else []\n",
    "    return pd.DataFrame(lines, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08925dbe",
   "metadata": {},
   "source": [
    "## Part 2: Guided Exercise ‚Äî Latvia Weather Data (Extra Messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e5dda",
   "metadata": {},
   "source": [
    "**Duration:** ~30 minutes  \n",
    "**Dataset:** `latvia_meteo_1925_extra_messy.zip`  \n",
    "**URL:** https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_extra_messy.zip\n",
    "\n",
    "### üéØ Objective\n",
    "Convert multiple extra-messy weather text files into **one cleaned file per source**, then load each into a **separate DataFrame**.\n",
    "\n",
    "### ‚úÖ Success Criteria\n",
    "- Each original text file has a corresponding `.good.txt` output.\n",
    "- Each `.good.txt` loads into a DataFrame without errors.\n",
    "- Basic column consistency achieved (same number of columns and sensible types where possible).\n",
    "\n",
    "### üîç What to Watch For\n",
    "- Junk header/footer lines (e.g., comments, separators)\n",
    "- Inconsistent separators (`,`, `;`, tabs, or spaces)\n",
    "- Missing fields and short/empty lines\n",
    "- Non-UTF8 characters ‚Äî use `errors='replace'` if needed\n",
    "\n",
    "### üß≠ Suggested Workflow\n",
    "1) **Download & unzip** to `data/` using `download_and_unzip`  \n",
    "2) **List files** and do a quick **line count** with `count_lines`  \n",
    "3) **Clean** with `clean_files(data_dir)`  \n",
    "4) **Load** each cleaned file with `load_cleaned_file`  \n",
    "5) **Sanity-check**: `.head()`, `.info()`, and simple value counts on key columns\n",
    "\n",
    "### üß© Hints\n",
    "- If a file still fails to parse, adjust `is_good_line` (e.g., skip lines that start with specific tokens).\n",
    "- If different files use different separators, handle at **pandas** stage later (Part 3) by re-parsing columns.\n",
    "- Keep outputs organized: write cleaned files into a `data/cleaned/` subfolder if you choose to extend `clean_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SKELETON (students fill in) ---\n",
    "EXTRA_URL = 'https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_extra_messy.zip'\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "# 1) Download & unzip\n",
    "# download_and_unzip(EXTRA_URL, DATA_DIR)\n",
    "\n",
    "# 2) Inspect: list files & counts\n",
    "# for p in sorted(DATA_DIR.glob('*.txt')):\n",
    "#     print(p.name, '->', count_lines(p))\n",
    "\n",
    "# 3) Clean all files\n",
    "# results = clean_files(DATA_DIR)\n",
    "# results\n",
    "\n",
    "# 4) Load cleaned files\n",
    "# dfs_extra = {}\n",
    "# for p in sorted(DATA_DIR.glob('*.good.txt')):\n",
    "#     dfs_extra[p.stem] = load_cleaned_file(p)\n",
    "# {k: v.head() for k, v in dfs_extra.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6fd3a",
   "metadata": {},
   "source": [
    "### üß™ Checkpoints\n",
    "- At least **N‚â•3** cleaned files successfully load into DataFrames.\n",
    "- No parsing exceptions on `.head()` or `.info()`.\n",
    "- You can explain (in comments) which rules your `is_good_line` used.\n",
    "\n",
    "### üõ† Extension (Optional)\n",
    "- Write a variant `clean_files(folder, out_dir=Path('data/cleaned'))` that writes outputs into a subfolder.\n",
    "- Add a **regex-based** `is_good_line_regex` that only keeps lines starting with `YYYY-MM-DD`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a410b",
   "metadata": {},
   "source": [
    "## Part 3: Pandas-Specific Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b764b0",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this section, you will standardize each DataFrame from Part 2 so they share a **common schema** and are ready to merge.\n",
    "\n",
    "### Target Schema (example)\n",
    "- `date` (datetime)\n",
    "- `station` (string/category)\n",
    "- `t_min` (float)\n",
    "- `t_max` (float)\n",
    "- `precip` (float)\n",
    "\n",
    "### Typical Operations\n",
    "1. **Column detection & renaming** ‚Äì bring different column names to a shared set\n",
    "2. **Type coercion** ‚Äì numbers via `pd.to_numeric(errors='coerce')`, dates via `pd.to_datetime(errors='coerce')`\n",
    "3. **Missing values** ‚Äì `dropna` or `fillna` depending on context\n",
    "4. **Duplicates** ‚Äì `.duplicated()` + `.drop_duplicates()`\n",
    "5. **Categoricals** ‚Äì normalize text (`strip`, `title`, `upper`) and `astype('category')` if useful\n",
    "6. **Validation** ‚Äì quick assertions (e.g., date not null, temperature ranges plausible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972b8ec",
   "metadata": {},
   "source": [
    "### Step-by-Step Guide\n",
    "1) **Pick one DataFrame** from `dfs_extra` and print `.head()`, `.columns`, `.info()`\n",
    "2) **Map columns** to target names (e.g., `temp_min` ‚Üí `t_min`)\n",
    "3) **Coerce**:\n",
    "   - `date = pd.to_datetime(df['date'], errors='coerce')`\n",
    "   - `df[['t_min','t_max','precip']] = df[['t_min','t_max','precip']].apply(pd.to_numeric, errors='coerce')`\n",
    "4) **Handle missing**: start conservative (e.g., drop rows missing `date` or all temperature columns)\n",
    "5) **Standardize station names**: `df['station'] = df['station'].astype(str).str.strip().str.title()`\n",
    "6) **Check duplicates** and remove\n",
    "7) **Repeat** for all DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc449af",
   "metadata": {},
   "source": [
    "### Common Pitfalls & Tips\n",
    "- Treat ambiguous `-` or `NA` strings as missing (`na_values=[\"-\",\"NA\",\"N/A\"]` if you re-read with `read_csv`)\n",
    "- Some files might have **merged columns**; split using `.str.split(',', expand=True)` when necessary\n",
    "- If a file lacks a column, create it with `pd.NA` so the schema lines up later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe28061",
   "metadata": {},
   "source": [
    "### üß± Skeleton: Inspect & Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example skeleton for one dataframe named df\n",
    "# df = dfs_extra['some_file']\n",
    "# print(df.head()); print(df.columns); df.info()\n",
    "\n",
    "# rename_map = {\n",
    "#     'Date': 'date', 'DATE':'date',\n",
    "#     'Station':'station', 'City':'station',\n",
    "#     'Tmin':'t_min', 'TminC':'t_min', 'Min':'t_min',\n",
    "#     'Tmax':'t_max', 'TmaxC':'t_max', 'Max':'t_max',\n",
    "#     'Precip':'precip', 'Rain':'precip'\n",
    "# }\n",
    "# df = df.rename(columns=lambda c: rename_map.get(str(c), str(c).strip().lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b255cc",
   "metadata": {},
   "source": [
    "### üß± Skeleton: Type Coercion & Missing Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ef475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_cols = ['date','station','t_min','t_max','precip']\n",
    "# for c in required_cols:\n",
    "#     if c not in df.columns:\n",
    "#         df[c] = pd.NA\n",
    "\n",
    "# df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "# for c in ['t_min','t_max','precip']:\n",
    "#     df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# # Drop rows with no usable date\n",
    "# df = df.dropna(subset=['date'])\n",
    "\n",
    "# # Optional: fill precip missing with 0 if domain-appropriate\n",
    "# # df['precip'] = df['precip'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69ada3",
   "metadata": {},
   "source": [
    "### üß± Skeleton: Text Normalization & Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['station'] = df['station'].astype(str).str.strip().str.title()\n",
    "# before = len(df)\n",
    "# df = df.drop_duplicates()\n",
    "# print('Removed', before - len(df), 'duplicate rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db547327",
   "metadata": {},
   "source": [
    "### üß™ Suggested Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67435a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert df['date'].notna().all(), 'Null dates remain'\n",
    "# # Optional plausibility checks (adjust to real units)\n",
    "# assert (df['t_min'] <= df['t_max']).dropna().all(), 'Found t_min > t_max'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1e041",
   "metadata": {},
   "source": [
    "## Part 4: Merging Cleaned DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d818314",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Combine all standardized DataFrames into **one big DataFrame** with a **unified column structure**.\n",
    "\n",
    "### Strategy\n",
    "1. **Define the target schema** used in Part 3.\n",
    "2. **Align each DataFrame** to the schema (add missing columns, reorder).\n",
    "3. **Concatenate** with `pd.concat`.\n",
    "4. **Final cleanup**: deduplicate, reindex, and sort by date/station.\n",
    "5. **Save outputs** (`CSV` or `Parquet`) for Day 3 (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee0bea",
   "metadata": {},
   "source": [
    "### Integration Checklist\n",
    "- All DataFrames have columns: `date, station, t_min, t_max, precip`\n",
    "- Dtypes are consistent across DataFrames\n",
    "- No catastrophic loss of rows during coercion\n",
    "- Final row count equals the sum of inputs minus duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d282b",
   "metadata": {},
   "source": [
    "### üß± Skeleton: Alignment & Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de996ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a dict of cleaned dfs: dfs_clean\n",
    "# target_cols = ['date','station','t_min','t_max','precip']\n",
    "\n",
    "# def coerce_to_schema(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "#     for c in cols:\n",
    "#         if c not in df.columns:\n",
    "#             df[c] = pd.NA\n",
    "#     # Reorder and drop extras for now\n",
    "#     return df[cols]\n",
    "\n",
    "# aligned = [coerce_to_schema(d.copy(), target_cols) for d in dfs_clean.values()]\n",
    "# big = pd.concat(aligned, axis=0, ignore_index=True)\n",
    "# big = big.drop_duplicates().reset_index(drop=True)\n",
    "# big = big.sort_values(['date','station'])\n",
    "# big.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2821858",
   "metadata": {},
   "source": [
    "### üßæ Export for Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = Path('outputs'); out_dir.mkdir(exist_ok=True)\n",
    "# big.to_csv(out_dir / 'latvia_meteo_1925_cleaned_merged.csv', index=False)\n",
    "# # Optional: Parquet for speed/size\n",
    "# # big.to_parquet(out_dir / 'latvia_meteo_1925_cleaned_merged.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259a98e",
   "metadata": {},
   "source": [
    "## üîÑ Reflection\n",
    "- What kinds of messiness were easier to fix with **Python basics**?\n",
    "- What kinds of messiness required **pandas**?\n",
    "- What are the risks of ‚Äúover-cleaning‚Äù or discarding too much data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
