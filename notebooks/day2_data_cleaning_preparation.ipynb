{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c5508c77",
      "metadata": {
        "id": "c5508c77"
      },
      "source": [
        "# üìÖ Day 2 ‚Äî Data Cleaning & Preparation\n",
        "\n",
        "*RTU Data Analysis & Visualization CPD course*\n",
        "\n",
        "**üìö Instruction (3h)**  \n",
        "- üßπ Handling missing values  \n",
        "- üóë Removing duplicates  \n",
        "- üîÑ Data type conversion  \n",
        "- üìÖ Parsing dates  \n",
        "- üèó Feature engineering basics  \n",
        "- üîó Combining datasets  \n",
        "- üè∑ Intro to categorical encoding  \n",
        "\n",
        "**üõ† Practical (1h)**  \n",
        "- üßΩ Clean a messy dataset  \n",
        "- üîÄ Merge with a secondary dataset  \n",
        "\n",
        "**üîÑ Reflection (1h)**  \n",
        "- üßê Review: common pitfalls in cleaning  \n",
        "- üí¨ Discuss real-world cleaning challenges  \n",
        "- üìù Recap exercise: identify cleaning steps for a small example dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea319510",
      "metadata": {
        "id": "ea319510"
      },
      "source": [
        "## üéØ Goals for the Day\n",
        "- Strengthen Python basics (functions, loops, if/else, file handling)\n",
        "- Learn to process raw messy text files into usable form\n",
        "- Apply pandas methods to clean incomplete/messy data\n",
        "- Merge multiple datasets into a single unified dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d16816f",
      "metadata": {
        "id": "9d16816f"
      },
      "source": [
        "## üí° Motivation / Explanation\n",
        "\n",
        "### Introduction to Data Cleaning and Preparation\n",
        "\n",
        "- **Why cleaning is critical before analysis**\n",
        "  - Raw data is almost never ready for direct analysis\n",
        "  - Errors, inconsistencies, and missing information can distort results\n",
        "  - Proper cleaning ensures reliability, reproducibility, and trust in analysis outcomes\n",
        "\n",
        "- **Real-world examples of messy data**\n",
        "  - üßπ Handling missing values - Weather records with missing timestamps or corrupt values\n",
        "  - üó≥Ô∏è Survey responses with inconsistent categories (e.g., \"Male\", \"male\", \"M\")\n",
        "  - üóë Removing duplicates - Financial transactions with duplicate entries\n",
        "  - üóë Log files with noise lines, system messages, or broken encodings\n",
        "  - üìÖ Parsing dates - Event logs with inconsistent timestamp formats\n",
        "  - üîÑ Data type conversion - User age recorded as text instead of numbers\n",
        "\n",
        "> Think of data cleaning as *‚Äúwashing vegetables before cooking‚Äù* ‚Äî not exciting, but essential for a good meal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5432cd86",
      "metadata": {
        "id": "5432cd86"
      },
      "source": [
        "## Weather Dataset: `latvia_meteo_1925_messy.zip`\n",
        "\n",
        "Let us imagine we are helping Toms Bricis with weather data analysis for the year 1925. We have come across a bundle of messy text files that require cleaning and preparation.\n",
        "\n",
        "- **What it is:** a bundle of **five ‚Äúmessy‚Äù text files** (‚âà50 rows each) simulating daily measurements from Latvian stations in **1925**:\n",
        "  - **Rƒ´ga-University** ‚Äî *Period 1 (Jan‚ÄìMar)*\n",
        "  - **Rƒ´ga-University** ‚Äî *Period 2 (Sep‚ÄìNov)*\n",
        "  - **LiepƒÅja** ‚Äî *Apr‚ÄìJun*\n",
        "  - **Mƒìrsrags** ‚Äî *Feb‚ÄìMay*\n",
        "  - **Al≈´ksne** ‚Äî *Oct‚ÄìDec*\n",
        "\n",
        "- **Columns present (but order varies by file):**  \n",
        "  `date`, `t_max_c`, `t_min_c`, `precip_24h_mm`, `precip_type`, `present_weather_code`, `notes`\n",
        "\n",
        "- **Deliberate ‚Äúmessiness‚Äù to practice cleaning:**\n",
        "  - **Different separators:** `;`, `,`, `|`, and **TAB** (documented in each file‚Äôs `# fields=` header).\n",
        "  - **Mixed column order** across files (use the header to map columns).\n",
        "  - **Date formats vary** (`YYYY-MM-DD`, `DD.MM.YYYY`, `YYYY/MM/DD`, `DD-MM-YYYY`, `MM-DD-YYYY`, `YYYY.MM.DD`) and sometimes **include a time** (e.g., `07:00`).\n",
        "  - **Numeric quirks:** decimal **commas** (e.g., `0,6`), **units** in strings (e.g., `0.8 mm`), and the Latvian word **‚Äúnulle‚Äù** for zero.\n",
        "  - **Missing values** sprinkled in as `\"\"`, `NA`, `‚Äî`, `-999`.\n",
        "  - **Codes as strings** with possible leading zeros (e.g., `present_weather_code = \"05\"`).\n",
        "  - **Free-text `notes`** in Latvian from the station master (may be blank/missing).\n",
        "\n",
        "- **Intended skills to practice (Day 2):**\n",
        "  - Detect & use **separators/column order** from headers.\n",
        "  - **Parse heterogeneous dates** (with optional times).\n",
        "  - Normalize **numerics/units** (decimal commas, `mm`, worded zeros).\n",
        "  - Unify **missing values** and enforce **types** (e.g., cast weather codes to integers).\n",
        "  - Keep useful **categorical text** (`precip_type`, `notes`) intact.\n",
        "\n",
        "As part of our workflow we will want to verify whether the above descriptions of messiness hold true for our specific dataset files. This will help us tailor our cleaning approach effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec9faea",
      "metadata": {
        "id": "eec9faea"
      },
      "source": [
        "## Part 1: Python Fundamentals for Data Cleaning\n",
        "\n",
        "For our first part we will use basic Python programming skills to explore and understand the dataset structure before diving into the cleaning process.\n",
        "\n",
        "### üîë Key Idea - Loops Go Brrr\n",
        "\n",
        "One of key advantages of programming is that we can automate repetitive tasks using loops. This is especially useful when working with datasets, as it allows us to apply the same operations to multiple rows or files without having to write redundant code.\n",
        "\n",
        "Similarly loops let us figure out an approach that works for a single file and then easily adapt it to others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ecad9f",
      "metadata": {
        "id": "85ecad9f"
      },
      "source": [
        "### Getting ready for work\n",
        "\n",
        "Typically in a finished notebook (and also normal scripts / programs), we want to start with a clear setup phase. This includes:\n",
        "\n",
        "1. **Importing Libraries:** Load all necessary libraries at the beginning.\n",
        "2. **Setting Up Paths:** Define file paths and other constants.\n",
        "3. **Configuring Options:** Set any options or preferences (e.g., display settings).\n",
        "\n",
        "By organizing our code this way, we make it easier to understand and modify later on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ca0466",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ca0466",
        "outputId": "ad1d3c6a-5abf-4cda-f28a-346985c0342c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Today : 2025-08-21T17:23:04\n",
            "Python : 3.12.5 (tags/v3.12.5:ff3bc82, Aug  6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)]\n",
            "pandas: 2.3.1\n",
            "pandas: 2.3.1\n",
            "Runtime: Windows-10-10.0.19045-SP0\n",
            "Total Current Drive Space: 273.69 GB\n",
            "Free Current Drive Space: 39.50 GB\n",
            "Current Working Directory: d:\\Github\\RTU_Data_Analysis_Visualization_CPD\\notebooks\n",
            "Runtime: Windows-10-10.0.19045-SP0\n",
            "Total Current Drive Space: 273.69 GB\n",
            "Free Current Drive Space: 39.50 GB\n",
            "Current Working Directory: d:\\Github\\RTU_Data_Analysis_Visualization_CPD\\notebooks\n"
          ]
        }
      ],
      "source": [
        "# usually we start with general Python imports\n",
        "from pathlib import Path # for file and file path related tasks\n",
        "import sys, platform, os, io, shutil, zipfile, re # system related tasks\n",
        "from datetime import datetime\n",
        "# first datetime\n",
        "print(f\"Today : {datetime.now().isoformat(timespec='seconds')}\")\n",
        "# now Python version\n",
        "print(f\"Python : {sys.version}\")\n",
        "\n",
        "# then we import external libraries\n",
        "# external - not part of Python installation\n",
        "# on Google Colab those are already installed\n",
        "try:\n",
        "    import pandas as pd\n",
        "    print('pandas:', pd.__version__)\n",
        "except ImportError:\n",
        "    print(f\"pandas not installed. Install with `pip install pandas`.\")\n",
        "    # for excel support extra instructions\n",
        "    print(f\"Install `openpyxl` for Excel support with `pip install openpyxl`.\")\n",
        "# requests is a widely used network library that makes internet \"requests\" easier\n",
        "try:\n",
        "    import requests\n",
        "except ImportError:\n",
        "    requests = None\n",
        "    print('requests not installed. Install with `pip install requests`.')\n",
        "\n",
        "\n",
        "# we can also print out what type of environment we are running in, this could show OS information\n",
        "print('Runtime:', platform.platform())\n",
        "# We could show system RAM and free RAM but that would require either a non standard library\n",
        "# or we would have to write some extra functions we skip this for now\n",
        "# you can ask LLM to write these functions for you\n",
        "# alternatively there are external libraries like psutil that do this out of the box\n",
        "\n",
        "# Let us show our current drive space\n",
        "print(f\"Total Current Drive Space: {shutil.disk_usage('/').total / (1024**3):.2f} GB\")\n",
        "print(f\"Free Current Drive Space: {shutil.disk_usage('/').free / (1024**3):.2f} GB\")\n",
        "\n",
        "# Current Working Directory\n",
        "print(f\"Current Working Directory: {Path.cwd()}\")\n",
        "# note that in some cases you might not want to provide all this information to the public, if you have a super secret computer...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e523062c",
      "metadata": {
        "id": "e523062c"
      },
      "source": [
        "### üßë‚Äçüíª Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dff1d21",
      "metadata": {
        "id": "6dff1d21"
      },
      "source": [
        "### What is the idea behind functions in programming?\n",
        "\n",
        "In programming, a function is a block of reusable code that performs a specific task. Think of it like a miniature program within your main program. Functions are designed to:\n",
        "\n",
        "- **Break down complex problems:** Large problems can be divided into smaller, manageable parts, each handled by a function. This makes code easier to write, understand, and debug.\n",
        "- **Avoid repetition (DRY principle - Don't Repeat Yourself):** If you need to perform the same set of actions multiple times, you can define a function once and call it whenever needed, rather than writing the same code repeatedly.\n",
        "- **Improve code organization and readability:** Functions group related code together, making the overall structure of your program clearer and easier to follow.\n",
        "- **Enhance code reusability:** Once a function is defined, it can be used in different parts of the same program or even in other programs.\n",
        "- **Simplify debugging:** If there's an issue, you can isolate the problem to a specific function, making it easier to find and fix the error.\n",
        "\n",
        "In essence, functions are tools for modularity and abstraction in programming, allowing you to create more organized, efficient, and maintainable code.\n",
        "\n",
        "### What is a function in Python?\n",
        "\n",
        "In Python, a function is defined using the `def` keyword, followed by the function name, parentheses `()`, and a colon `:`. The code block within the function is indented. Functions can optionally take inputs called *arguments* (placed inside the parentheses) and can return a value using the `return` keyword.\n",
        "\n",
        "Here's a basic structure of a Python function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94c3040",
      "metadata": {
        "id": "d94c3040"
      },
      "outputs": [],
      "source": [
        "def greet(name: str = 'student') -> str:\n",
        "    \"\"\"\n",
        "    Returns a friendly greeting for the given name.\n",
        "    If no name is provided, defaults to 'student'.\n",
        "    \"\"\"\n",
        "    # Use an f-string to insert the name into the greeting\n",
        "    # note we are not printing the greeting here, just returning it for use by another part of the code\n",
        "    return f\"Hello, {name}!\" # we can insert pretty much any type of data in f-strings\n",
        "\n",
        "# also nothing should be happening besides the function being added to our memory\n",
        "# we have not called the function yet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0a8142",
      "metadata": {
        "id": "ed0a8142",
        "outputId": "db0287fe-ad8a-4a11-a095-36fd90f8f4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, student!\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "greeting = greet() # assign results of greet() function to greeting variable\n",
        "print(greeting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DGWKzOyzG-BT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGWKzOyzG-BT",
        "outputId": "d143ab90-7114-4b2c-e357-c1cd2d2ac989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, Valdis!\n",
            "Hello, 808!\n"
          ]
        }
      ],
      "source": [
        "# Now that I have this function I can make other greetings\n",
        "my_greeting = greet(\"Valdis\")\n",
        "print(my_greeting)\n",
        "numeric_greeting = greet(808) # function expects str, but no penalty in this case\n",
        "print(numeric_greeting)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea45c37c",
      "metadata": {
        "id": "ea45c37c"
      },
      "source": [
        "### Type Hints\n",
        "\n",
        "Type hints in Python provide a way to indicate the expected data types of variables, function parameters, and return values. They help improve code readability and enable better static analysis by tools like linters and IDEs.\n",
        "\n",
        "However, they have no actual power at runtime and are not enforced by the Python interpreter. They serve as a guideline for developers and can be checked using static type checkers like mypy.\n",
        "\n",
        "I liken them to \"documentation for your code.\" Just as documentation helps users understand how to use your code, type hints help developers understand what types of values are expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7e7b4b",
      "metadata": {
        "id": "ad7e7b4b"
      },
      "outputs": [],
      "source": [
        "# let's see one more example with simple add function\n",
        "def add(a: int, b: int) -> int:\n",
        "    return a + b\n",
        "\n",
        "# so this function expects only integers and returns integer\n",
        "\n",
        "# however it will work with any values that support adding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4c906f",
      "metadata": {
        "id": "9d4c906f",
        "outputId": "e76c8be5-ad43-4d2b-f6fa-ae8b54c9b934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "5.85\n",
            "Valdis RTU\n"
          ]
        }
      ],
      "source": [
        "print(add(2,2))\n",
        "print(add(3.14, 2.71))\n",
        "print(add(\"Valdis\", \" RTU\"))\n",
        "\n",
        "# so again type hints are useful (AI will happily make them for us)\n",
        "# but in Python they only serve as a guiding light"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eE3hLZ5nIBAX",
      "metadata": {
        "id": "eE3hLZ5nIBAX"
      },
      "source": [
        "### Function to download and unzip file from url\n",
        "\n",
        "Below is a more complicated function that does two things: it downloads a zip file from a given URL and then extracts its contents to a specified directory.\n",
        "\n",
        "Theoretically speaking it would be better to have two functions one that downloads the file and another that extracts it.\n",
        "\n",
        "In general functions should do one thing and do it well.\n",
        "\n",
        "When you need to do more than one thing you can combine them into a single function, but be mindful of keeping the function focused and not overly complex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b2f894",
      "metadata": {
        "id": "f4b2f894"
      },
      "outputs": [],
      "source": [
        "def download_and_unzip(url: str, target_folder: str | Path = 'sample_data') -> Path:\n",
        "    \"\"\"\n",
        "    Downloads a ZIP file from the given URL and extracts it to the target folder.\n",
        "    Returns the path to the folder where files were extracted.\n",
        "    \"\"\"\n",
        "    target = Path(target_folder)  # Make sure target is a Path object\n",
        "    target.mkdir(parents=True, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "    filename = url.split('/')[-1]  # Get the file name from the URL\n",
        "    # in case of URL the file name is the last one (so index -1 means last one in a list)\n",
        "    # next we create full path where we will save the zip file\n",
        "    zip_path = target / filename  # Full path to save the ZIP file\n",
        "    # check if library exists\n",
        "    if requests is None:\n",
        "        raise RuntimeError('requests required.')\n",
        "    # Download the file in chunks (good for large files)\n",
        "    with requests.get(url, stream=True, timeout=60) as r:\n",
        "        r.raise_for_status()  # Raise an error if download failed\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    # Unzip the downloaded file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(target) # this command given a zip file as target will unzip ALL files\n",
        "    # Optionally, remove the ZIP file after extraction\n",
        "    # zip_path.unlink(missing_ok=True)  # Remove the ZIP file\n",
        "    return target  # Return the folder where files were extracted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "369862d6",
      "metadata": {
        "id": "369862d6"
      },
      "source": [
        "**Practice dataset for Part 1:** `latvia_meteo_1925_messy.zip` (5 text files)\n",
        "\n",
        "- URL: https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_messy.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c941a28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c941a28",
        "outputId": "657b2a70-18d5-4e18-db85-ac2754c92b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will download and unzip from following url: https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_messy.zip\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WindowsPath('day_2_data')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's download the messy files zip\n",
        "url = \"https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_messy.zip\"\n",
        "print(f\"Will download and unzip from following url: {url}\")\n",
        "# let's download the file and extract it under day_2_data\n",
        "download_and_unzip(url, Path(\"day_2_data\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zVxkQR7yK88c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zVxkQR7yK88c",
        "outputId": "adb268e9-bae0-44a2-def1-aa51bafce90c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_03b3df5e-6882-4766-8e32-32f6388aad45\", \"aluksne_1925.txt\", 2033)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_8abf2ed8-fd22-408f-82a8-2b986c8ae230\", \"liepaja_1925.txt\", 2077)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_8b1ba123-78d4-4ea8-831a-c7eaca540ea0\", \"mersrags_1925.txt\", 2537)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_60f84e3a-e9fe-4e48-b82e-b95b7a3818c4\", \"riga_university_1925_p1.txt\", 2006)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4dd17be7-ca37-492d-aded-b78fef4bde5a\", \"riga_university_1925_p2.txt\", 1908)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# let's add Google Colab specific code that will offer download to your computer of all the files extracted\n",
        "# this will not work locally since you already have these files locally!\n",
        "from google.colab import files # this is Google Colab specific\n",
        "# if you were local you would already have these files locally\n",
        "for p in sorted(Path(\"day_2_data\").glob(\"*.txt\")): # glob looks in current folder\n",
        "    files.download(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b153f8c3",
      "metadata": {
        "id": "b153f8c3"
      },
      "source": [
        "### üìÇ File Handling\n",
        "\n",
        "First let me show you how to read whole text file into one big text string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2M3mbQ_PZCq",
      "metadata": {
        "id": "B2M3mbQ_PZCq"
      },
      "outputs": [],
      "source": [
        "# let's make a function that takes a file Path or string and optional encoding with default utf-8 and returns text string\n",
        "def get_file_contents(path: Path | str, encoding: str = 'utf-8') -> str:\n",
        "    \"\"\"\n",
        "    Reads the contents of a text file and returns it as a string.\n",
        "    \"\"\"\n",
        "    # so path could be string or Path\n",
        "    # file could have any extension but it should contain text of some sort\n",
        "    with open(path, 'r', encoding=encoding) as f:\n",
        "        content = f.read()\n",
        "    # here file is automatically closed after the with block\n",
        "    return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qwAhyXkoP19P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwAhyXkoP19P",
        "outputId": "f977acb3-d252-4053-fd66-0d47a656b45d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# station_name=Al≈´ksne\n",
            "# period=(Oct‚ÄìDec)\n",
            "# separator_hint=|\n",
            "# columns_in_this_file= notes | present_weather_code | t_max_c | precip_24h_mm | date | precip_type | t_min_c\n",
            "# note: some values intentionally messy (units, words, missing, time in date)\n",
            "# fields=notes|present_weather_code|t_max_c|precip_24h_mm|date|precip_type|t_min_c\n",
            "‚Äî|53|5.9|3.0 mm|10-19-1925|R|5.5\n",
            "Rƒ´ts auksts|-999|9.1|2.3 mm|18.10.1925|mixed|4.2\n",
            "Daƒºƒìji mƒÅko≈Üains|65|5.4|0.0|11-10-1925||-0.5\n",
            "Daƒºƒìji mƒÅko≈Üains|50|15.6|0.5|10-07-1925 14:00|S|10.4\n",
            "Neliels vƒìj≈°|63|-1.1|0.0|12-28-1925|‚Äî|‚Äî\n",
            "VƒìlƒÅk sƒÅka lƒ´t|-999|5.3||12-07-1925 19:00|none|\n",
            "P≈´tis brƒÅzmas|82|4.2|0.6 mm|11-07-1925|S|0.1\n",
            "‚Äî|61|-7.2|1.4|12-03-1925|mixed|-6.5\n",
            "Laikam migla|51|3.3|1.0 mm|11-27-1925|mixed|-1.3\n",
            "Mƒìrƒ´≈°anas kƒº≈´da?|70|-3.5999999999999996|2.1|12-06-1925|M|-2.9\n",
            "Skurste≈Üi k≈´p|71|6.2|0.0|10-21-1925||6.9\n",
            "NA|61|1.3|2.1|28.11.1925|NA|2.0\n",
            "Ap pusdienlaiku saule|80|8.4|1.9 mm|02-11-1925|snow|-1.5\n",
            "|53|8.4||10-01-1925 07:00||3.3\n",
            "Mƒìrƒ´jums apstiprinƒÅts|53|10.8|0.7 mm|10-08-1925|S|5.3\n",
            "Rƒ´ts auksts|45|8.1|0.1|1925-11-25|NA|-999\n",
            "Laikam migla|53|8.9|2.5|10-17-1925 07:00|R|7.1\n",
            "NA||5.4|0.8|11-22-1925 19:00|snow|2.7\n",
            "|82|1.1|0.9|12-25-1925 08:00|S|-4.5\n",
            "‚Äî|65||0.0 mm|10-23-1925|none|-0.4\n",
            "Daƒºƒìji mƒÅko≈Üains|60|9.1|1.7|10-02-1925|S|1.6\n",
            "P≈´tis brƒÅzmas|80|10.3|nulle|10-16-1925 14:00|‚Äî|7.9\n",
            "P≈´tis brƒÅzmas|61|6.0|0.2 mm|11-24-1925 14:00|snow|-1.9\n",
            "Mƒìrƒ´jums apstiprinƒÅts|45|2.9|1.1 mm|12-14-1925 19:00||-6.6\n",
            "Laikam migla|82|9.0|0.0|10-10-1925||3.9\n",
            "|63|3.9|1.7 mm|12-02-1925|S|-2.3\n",
            "Ap pusdienlaiku saule|81|2.3|0.0 mm|11-03-1925 20:00|NA|1.4\n",
            "‚Äî|53|-3.0|0.6|12-13-1925|mixed|-4.0\n",
            "Rƒ´ts auksts|73|12.3|1.9|10-20-1925|mixed|-999\n",
            "Daƒºƒìji mƒÅko≈Üains|51||3.5 mm|11-06-1925 08:00|M|-4.6\n",
            "VƒìlƒÅk sƒÅka lƒ´t|60|NA|1.1 mm|11-20-1925|M|1.8\n",
            "Mƒìrƒ´jums apstiprinƒÅts|51|-2.2|0.2 mm|12-18-1925|snow|-7.6\n",
            "P≈´tis brƒÅzmas|82|5.3||11-18-1925||-0.4\n",
            "NA|63|3.4||12-19-1925|M|-6.0\n",
            "Novƒìrojums vƒìlƒÅk|90|-0.2|2.3|12-22-1925|mixed|-8.2\n",
            "Daƒºƒìji mƒÅko≈Üains|90|-3.5|0.2 mm|12-16-1925|rain|-4.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's read aluksne into memory\n",
        "# aluksne = get_file_contents(Path(\"day_2_data/aluksne_1925.txt\"))\n",
        "aluksne = get_file_contents(\"day_2_data/aluksne_1925.txt\")\n",
        "print(aluksne)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xu2zQhslRDSd",
      "metadata": {
        "id": "Xu2zQhslRDSd"
      },
      "source": [
        "### Iterating over file line at a time\n",
        "\n",
        "Above example showed how we could read a whole file into memory.\n",
        "However that means we would be working with file as one big string as a whole. We could do some replace operations.\n",
        "\n",
        "Much more often we will want to work on file one line(row) at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87dc72f9",
      "metadata": {
        "id": "87dc72f9"
      },
      "source": [
        "### üîÑ For Loops\n",
        "\n",
        "For loops in Python let us iterate over a sequence (like a list, tuple, or string, or other iterables such as lines in a file) and perform an action for each item in that sequence.\n",
        "\n",
        "General syntax of for loops is:\n",
        "\n",
        "```Python\n",
        "for element in <iterable>:\n",
        "    <action>\n",
        "    <more optional action>\n",
        "```\n",
        "\n",
        "Note the indentation, as usual in Python after : we have indentation to indicate the block of code that belongs to the for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae3f7301",
      "metadata": {
        "id": "ae3f7301"
      },
      "outputs": [],
      "source": [
        "def count_lines(path: Path | str) -> int:\n",
        "    \"\"\"\n",
        "    Counts the total number of lines and the number of non-empty lines in a file.\n",
        "    Returns a tuple: (total_lines, nonempty_lines)\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        for line in f:\n",
        "            total += 1\n",
        "            # we could do something more here with the line but here we just count the contents\n",
        "\n",
        "    return total # Return the line count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lOyraEVYRpDS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOyraEVYRpDS",
        "outputId": "97cf47a0-0aec-45ea-ad06-88aea12b292d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's count lines in aluksne_1925\n",
        "count_lines(Path(\"day_2_data/aluksne_1925.txt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb1b335",
      "metadata": {
        "id": "0cb1b335"
      },
      "source": [
        "## Determining the separator character\n",
        "\n",
        "Our files have helpfully provided a separator hint without which it would be very hard to determine.\n",
        "\n",
        "`# separator_hint=|`\n",
        "`# separator_hint=TAB`\n",
        "`# separator_hint=,`\n",
        "\n",
        "Let's write a function that extracts hint from the file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f354afe",
      "metadata": {
        "id": "5f354afe"
      },
      "outputs": [],
      "source": [
        "def get_sep(path: Path) -> str:\n",
        "    \"\"\"\n",
        "    We look for first line that contains\n",
        "    `# separator_hint=|`\n",
        "`# separator_hint=TAB`\n",
        "`# separator_hint=,`\n",
        "    \"\"\"\n",
        "    needle = \"separator_hint=\"\n",
        "    sep = None # we start with assumption that we do not know the separator\n",
        "    # lets open file and go through line by line\n",
        "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        for line in f:\n",
        "            if needle in line: # we check for presence of needle\n",
        "            # we could have used regular expression but no need here\n",
        "                # let's find whatever is after needle\n",
        "                # so we split by needle at take last part\n",
        "                sep = line.split(needle)[-1].strip()\n",
        "    # if sep is TAB we need to return \\t\n",
        "    if sep == \"TAB\": # special case TAB which we need to convert\n",
        "        return \"\\t\"\n",
        "    return sep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dMQsAaEyaCtd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dMQsAaEyaCtd",
        "outputId": "2ffc63f6-d731-462e-96be-78891432c692"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'|'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's see if we can find separator for aluksne_1925\n",
        "get_sep(Path(\"day_2_data/aluksne_1925.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XvUvlg5zaRzg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvUvlg5zaRzg",
        "outputId": "bc0d2e4f-3e1e-495e-8414-d3ff8f7fb24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day_2_data\\aluksne_1925.txt |\n",
            "day_2_data\\liepaja_1925.txt ,\n",
            "day_2_data\\mersrags_1925.txt \t\n",
            "day_2_data\\riga_university_1925_p1.txt ;\n",
            "day_2_data\\riga_university_1925_p2.txt ;\n"
          ]
        }
      ],
      "source": [
        "# let's find it for all txt files that do not contain good or bad\n",
        "for p in sorted(Path(\"day_2_data\").glob(\"*.txt\")):\n",
        "    if \"good\" in p.name or \"bad\" in p.name: continue\n",
        "    # we could have used bad as well\n",
        "    print(p, get_sep(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "177a338f",
      "metadata": {
        "id": "177a338f"
      },
      "source": [
        "### üì• Loading Cleaned Files into DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf9cc54",
      "metadata": {
        "id": "8bf9cc54"
      },
      "outputs": [],
      "source": [
        "# Let us write a function that will take a file path\n",
        "# This function will go through file line by line\n",
        "# If line with separator_hint= is found we save in sep value the last part of this line after stripping whitespace\n",
        "# any other line that starts with hash (#) is ignored\n",
        "# all other lines are split using sep value and stored as a list of lists (2d)\n",
        "# once all lines are read the 2d list is converted to Dataframe and returned\n",
        "def load_messy_file(path: Path | str) -> pd.DataFrame:\n",
        "    sep = None\n",
        "    lines = [] # empty list\n",
        "    # with guarantees closing of file at the end\n",
        "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        # we loop through file one row at a time\n",
        "        for line in f:\n",
        "            if line.startswith('# separator_hint='):\n",
        "                # extract separator\n",
        "                sep = line.split('=')[-1].strip()\n",
        "                # add special case TAB -> \\t\n",
        "                if sep == \"TAB\":\n",
        "                    sep = \"\\t\"\n",
        "            if line.startswith(\"#\"): # order is important we check this AFTER hint check\n",
        "                continue # we go to next line\n",
        "            if sep:\n",
        "                # we split the row by separator and add it to our list of lines\n",
        "                lines.append(line.split(sep))\n",
        "    # Here file is closed - that's good and safe!\n",
        "    # all that remains is to create a dataframe from our lines\n",
        "    df = pd.DataFrame(lines) # we leave the challenge of creating appropriate names for later\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160242eb",
      "metadata": {
        "id": "160242eb"
      },
      "outputs": [],
      "source": [
        "# okay let's write a function that takes src folder containing text files\n",
        "# function takes also dst folder for output of xlsx files that will be obtained by saving Dataframes from text files\n",
        "def create_xlsx_files(src: Path | str, dst: Path | str):\n",
        "    # first read text files in src\n",
        "    src = Path(src) # to make sure that src is actually Path\n",
        "    text_files = sorted(src.glob('*.txt'))\n",
        "\n",
        "    # create dst if it does not exist\n",
        "    dst = Path(dst)\n",
        "    dst.mkdir(exist_ok=True)\n",
        "    # now simply loop through text_files and create Dataframe and save in dst as text file with xlsx suffix\n",
        "    for p in text_files:\n",
        "        df = load_messy_file(p)\n",
        "        df.to_excel(dst / (p.stem + \".xlsx\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc921bad",
      "metadata": {
        "id": "bc921bad"
      },
      "source": [
        "### Saving converted files into XLSX\n",
        "\n",
        "Now that we have needed functions we can use them to convert text files to XLSX format.\n",
        "\n",
        "Of course the text files should match the specific format (have separator hint and naturally have lines of content separated by that separtor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42291ec0",
      "metadata": {
        "id": "42291ec0"
      },
      "outputs": [],
      "source": [
        "# let'' take day_2_data text files\n",
        "# and convert them to xlsx\n",
        "# let's use new folder for that\n",
        "# let's call it day_2_xlsx\n",
        "INPUT_DIR = \"day_2_data\"\n",
        "OUTPUT_DIR = \"day_2_xlsx\"\n",
        "\n",
        "create_xlsx_files(INPUT_DIR, OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08925dbe",
      "metadata": {
        "id": "08925dbe"
      },
      "source": [
        "## Part 2: Guided Exercise ‚Äî Latvia Weather Data (Extra Messy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71e5dda",
      "metadata": {
        "id": "d71e5dda"
      },
      "source": [
        "**Duration:** ~30 minutes  \n",
        "**Dataset:** `latvia_meteo_1925_extra_messy.zip`  \n",
        "**URL:** https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_extra_messy.zip\n",
        "\n",
        "### üéØ Objective\n",
        "Convert multiple extra-messy weather text files   into XLSX files into a new folder of your choice\n",
        "\n",
        "### Hint\n",
        "\n",
        "Simply run the provided functions with folders of your choice.\n",
        "\n",
        "### Advanced Users\n",
        "\n",
        "You can start working on extracting column names from the original files. We will do that after break."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d0ea1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95d0ea1f",
        "outputId": "c7a52acd-5727-4e8d-d395-85abf1be85e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day_2_exercise/dobele_1925.txt\n",
            "day_2_exercise/mersrags_extra_1925.txt\n",
            "day_2_exercise/pavilosta_1925.txt\n"
          ]
        }
      ],
      "source": [
        "# --- SKELETON (students fill in) ---\n",
        "EXTRA_URL = 'https://github.com/ValRCS/RTU_Data_Analysis_Visualization_CPD/raw/refs/heads/main/data/latvia_meteo_1925_extra_messy.zip'\n",
        "DATA_DIR = Path('day_2_exercise')\n",
        "\n",
        "# 1) Download & unzip\n",
        "# download_and_unzip(EXTRA_URL, DATA_DIR)\n",
        "\n",
        "# # 2) Print contents of DATA DIR\n",
        "\n",
        "# data_files = sorted(DATA_DIR.glob('*.txt'))\n",
        "# for file_name in data_files: # file_names is just a variable name\n",
        "#   print(file_name)\n",
        "\n",
        "# 2) Inspect: list files & counts\n",
        "# for p in sorted(DATA_DIR.glob('*.txt')):\n",
        "#     print(p.name, '->', count_lines(p))\n",
        "\n",
        "# 3) Clean all files\n",
        "# results = clean_files(DATA_DIR)\n",
        "# results\n",
        "\n",
        "# 4) Load cleaned files\n",
        "# dfs_extra = {}\n",
        "# for p in sorted(DATA_DIR.glob('*.good.txt')):\n",
        "#     dfs_extra[p.stem] = load_cleaned_file(p)\n",
        "# {k: v.head() for k, v in dfs_extra.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YqgOCLyaxINo",
      "metadata": {
        "id": "YqgOCLyaxINo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dAVu73yyKuP",
      "metadata": {
        "id": "4dAVu73yyKuP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V_YIyJj_yAj_",
      "metadata": {
        "id": "V_YIyJj_yAj_"
      },
      "outputs": [],
      "source": [
        "# # let us test on day_2_exercise folder\n",
        "# INPUT_FOLDER = Path(\"day_2_exercise\") # this should actually exist\n",
        "# OUTPUT_FOLDER = Path(\"day_2_exercise_output\") # this can be any valid folder name\n",
        "# create_xlsx_files(INPUT_FOLDER, OUTPUT_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e6fd3a",
      "metadata": {
        "id": "96e6fd3a"
      },
      "source": [
        "### üß™ Checkpoints\n",
        "- At least **N‚â•3** cleaned files successfully load into DataFrames.\n",
        "- No parsing exceptions on `.head()` or `.info()`.\n",
        "- You can explain (in comments) which rules your `is_good_line` used.\n",
        "\n",
        "### üõ† Extension (Optional)\n",
        "- Write a variant `clean_files(folder, out_dir=Path('data/cleaned'))` that writes outputs into a subfolder.\n",
        "- Add a **regex-based** `is_good_line_regex` that only keeps lines starting with `YYYY-MM-DD`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a2569c",
      "metadata": {
        "id": "c1a2569c"
      },
      "source": [
        "## Part 2b: Extracting column names automatically\n",
        "\n",
        "We were able to extract the data from the files and load them into DataFrames and then save them into XLSX files.\n",
        "\n",
        "One negative is that we do not have column names just numbers.\n",
        "\n",
        "Thus we would like to extract column names automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f89a410b",
      "metadata": {
        "id": "f89a410b"
      },
      "source": [
        "## Part 3: Pandas-Specific Data Cleaning\n",
        "\n",
        "Now we will focus on cleaning the data using pandas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b764b0",
      "metadata": {
        "id": "99b764b0"
      },
      "source": [
        "### Overview\n",
        "In this section, you will standardize each DataFrame from Part 2 so they share a **common schema** and are ready to merge.\n",
        "\n",
        "### Target Schema (example)\n",
        "- `date` (datetime)\n",
        "- `station` (string/category)\n",
        "- `t_min` (float)\n",
        "- `t_max` (float)\n",
        "- `precip` (float)\n",
        "\n",
        "### Typical Operations\n",
        "1. **Column detection & renaming** ‚Äì bring different column names to a shared set\n",
        "2. **Type coercion** ‚Äì numbers via `pd.to_numeric(errors='coerce')`, dates via `pd.to_datetime(errors='coerce')`\n",
        "3. **Missing values** ‚Äì `dropna` or `fillna` depending on context\n",
        "4. **Duplicates** ‚Äì `.duplicated()` + `.drop_duplicates()`\n",
        "5. **Categoricals** ‚Äì normalize text (`strip`, `title`, `upper`) and `astype('category')` if useful\n",
        "6. **Validation** ‚Äì quick assertions (e.g., date not null, temperature ranges plausible)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1972b8ec",
      "metadata": {
        "id": "1972b8ec"
      },
      "source": [
        "### Step-by-Step Guide\n",
        "1) **Pick one DataFrame** from `dfs_extra` and print `.head()`, `.columns`, `.info()`\n",
        "2) **Map columns** to target names (e.g., `temp_min` ‚Üí `t_min`)\n",
        "3) **Coerce**:\n",
        "   - `date = pd.to_datetime(df['date'], errors='coerce')`\n",
        "   - `df[['t_min','t_max','precip']] = df[['t_min','t_max','precip']].apply(pd.to_numeric, errors='coerce')`\n",
        "4) **Handle missing**: start conservative (e.g., drop rows missing `date` or all temperature columns)\n",
        "5) **Standardize station names**: `df['station'] = df['station'].astype(str).str.strip().str.title()`\n",
        "6) **Check duplicates** and remove\n",
        "7) **Repeat** for all DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc449af",
      "metadata": {
        "id": "8dc449af"
      },
      "source": [
        "### Common Pitfalls & Tips\n",
        "- Treat ambiguous `-` or `NA` strings as missing (`na_values=[\"-\",\"NA\",\"N/A\"]` if you re-read with `read_csv`)\n",
        "- Some files might have **merged columns**; split using `.str.split(',', expand=True)` when necessary\n",
        "- If a file lacks a column, create it with `pd.NA` so the schema lines up later"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe28061",
      "metadata": {
        "id": "afe28061"
      },
      "source": [
        "### üß± Skeleton: Inspect & Rename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f16bfb3c",
      "metadata": {
        "id": "f16bfb3c"
      },
      "outputs": [],
      "source": [
        "# Example skeleton for one dataframe named df\n",
        "# df = dfs_extra['some_file']\n",
        "# print(df.head()); print(df.columns); df.info()\n",
        "\n",
        "# rename_map = {\n",
        "#     'Date': 'date', 'DATE':'date',\n",
        "#     'Station':'station', 'City':'station',\n",
        "#     'Tmin':'t_min', 'TminC':'t_min', 'Min':'t_min',\n",
        "#     'Tmax':'t_max', 'TmaxC':'t_max', 'Max':'t_max',\n",
        "#     'Precip':'precip', 'Rain':'precip'\n",
        "# }\n",
        "# df = df.rename(columns=lambda c: rename_map.get(str(c), str(c).strip().lower()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b255cc",
      "metadata": {
        "id": "d6b255cc"
      },
      "source": [
        "### üß± Skeleton: Type Coercion & Missing Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19ef475",
      "metadata": {
        "id": "a19ef475"
      },
      "outputs": [],
      "source": [
        "# required_cols = ['date','station','t_min','t_max','precip']\n",
        "# for c in required_cols:\n",
        "#     if c not in df.columns:\n",
        "#         df[c] = pd.NA\n",
        "\n",
        "# df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "# for c in ['t_min','t_max','precip']:\n",
        "#     df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "# # Drop rows with no usable date\n",
        "# df = df.dropna(subset=['date'])\n",
        "\n",
        "# # Optional: fill precip missing with 0 if domain-appropriate\n",
        "# # df['precip'] = df['precip'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e69ada3",
      "metadata": {
        "id": "6e69ada3"
      },
      "source": [
        "### üß± Skeleton: Text Normalization & Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6537d71d",
      "metadata": {
        "id": "6537d71d"
      },
      "outputs": [],
      "source": [
        "# df['station'] = df['station'].astype(str).str.strip().str.title()\n",
        "# before = len(df)\n",
        "# df = df.drop_duplicates()\n",
        "# print('Removed', before - len(df), 'duplicate rows')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db547327",
      "metadata": {
        "id": "db547327"
      },
      "source": [
        "### üß™ Suggested Sanity Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67435a09",
      "metadata": {
        "id": "67435a09"
      },
      "outputs": [],
      "source": [
        "# assert df['date'].notna().all(), 'Null dates remain'\n",
        "# # Optional plausibility checks (adjust to real units)\n",
        "# assert (df['t_min'] <= df['t_max']).dropna().all(), 'Found t_min > t_max'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc1e041",
      "metadata": {
        "id": "fbc1e041"
      },
      "source": [
        "## Part 4: Merging Cleaned DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d818314",
      "metadata": {
        "id": "2d818314"
      },
      "source": [
        "### Goal\n",
        "Combine all standardized DataFrames into **one big DataFrame** with a **unified column structure**.\n",
        "\n",
        "### Strategy\n",
        "1. **Define the target schema** used in Part 3.\n",
        "2. **Align each DataFrame** to the schema (add missing columns, reorder).\n",
        "3. **Concatenate** with `pd.concat`.\n",
        "4. **Final cleanup**: deduplicate, reindex, and sort by date/station.\n",
        "5. **Save outputs** (`CSV` or `Parquet`) for Day 3 (EDA)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ee0bea",
      "metadata": {
        "id": "65ee0bea"
      },
      "source": [
        "### Integration Checklist\n",
        "- All DataFrames have columns: `date, station, t_min, t_max, precip`\n",
        "- Dtypes are consistent across DataFrames\n",
        "- No catastrophic loss of rows during coercion\n",
        "- Final row count equals the sum of inputs minus duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a2d282b",
      "metadata": {
        "id": "4a2d282b"
      },
      "source": [
        "### üß± Skeleton: Alignment & Concatenation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de996ea9",
      "metadata": {
        "id": "de996ea9"
      },
      "outputs": [],
      "source": [
        "# Suppose you have a dict of cleaned dfs: dfs_clean\n",
        "# target_cols = ['date','station','t_min','t_max','precip']\n",
        "\n",
        "# def coerce_to_schema(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "#     for c in cols:\n",
        "#         if c not in df.columns:\n",
        "#             df[c] = pd.NA\n",
        "#     # Reorder and drop extras for now\n",
        "#     return df[cols]\n",
        "\n",
        "# aligned = [coerce_to_schema(d.copy(), target_cols) for d in dfs_clean.values()]\n",
        "# big = pd.concat(aligned, axis=0, ignore_index=True)\n",
        "# big = big.drop_duplicates().reset_index(drop=True)\n",
        "# big = big.sort_values(['date','station'])\n",
        "# big.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2821858",
      "metadata": {
        "id": "d2821858"
      },
      "source": [
        "### üßæ Export for Day 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f8911d",
      "metadata": {
        "id": "53f8911d"
      },
      "outputs": [],
      "source": [
        "# out_dir = Path('outputs'); out_dir.mkdir(exist_ok=True)\n",
        "# big.to_csv(out_dir / 'latvia_meteo_1925_cleaned_merged.csv', index=False)\n",
        "# # Optional: Parquet for speed/size\n",
        "# # big.to_parquet(out_dir / 'latvia_meteo_1925_cleaned_merged.parquet', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8259a98e",
      "metadata": {
        "id": "8259a98e"
      },
      "source": [
        "## üîÑ Reflection\n",
        "- What kinds of messiness were easier to fix with **Python basics**?\n",
        "- What kinds of messiness required **pandas**?\n",
        "- What are the risks of ‚Äúover-cleaning‚Äù or discarding too much data?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}